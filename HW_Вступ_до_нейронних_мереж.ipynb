{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YuliaLoburets/ML_learning/blob/main/HW_%D0%92%D1%81%D1%82%D1%83%D0%BF_%D0%B4%D0%BE_%D0%BD%D0%B5%D0%B9%D1%80%D0%BE%D0%BD%D0%BD%D0%B8%D1%85_%D0%BC%D0%B5%D1%80%D0%B5%D0%B6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Секція 1. Логістична регресія з нуля.**\n",
        "\n",
        "Будемо крок за кроком будувати модель лог регресії з нуля для передбачення, чи буде врожай більше за 80 яблук (задача подібна до лекційної, але на класифікацію).\n",
        "\n",
        "Давайте нагадаємо основні формули для логістичної регресії.\n",
        "\n",
        "### Функція гіпотези - обчислення передбачення у логістичній регресії:\n",
        "\n",
        "$$\n",
        "\\hat{y} = \\sigma(x W^T + b) = \\frac{1}{1 + e^{-(x W^T + b)}}\n",
        "$$\n",
        "\n",
        "Де:\n",
        "- $ \\hat{y} $ — це ймовірність \"позитивного\" класу.\n",
        "- $ x $ — це вектор (або матриця для набору прикладів) вхідних даних.\n",
        "- $ W $ — це вектор (або матриця) вагових коефіцієнтів моделі.\n",
        "- $ b $ — це зміщення (bias).\n",
        "- $ \\sigma(z) $ — це сигмоїдна функція активації.\n",
        "\n",
        "### Як обчислюється сигмоїдна функція:\n",
        "\n",
        "Сигмоїдна функція $ \\sigma(z) $ має вигляд:\n",
        "\n",
        "$$\n",
        "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "$$\n",
        "\n",
        "Ця функція перетворює будь-яке дійсне значення $ z $ в інтервал від 0 до 1, що дозволяє інтерпретувати вихід як ймовірність для логістичної регресії.\n",
        "\n",
        "### Формула функції втрат для логістичної регресії (бінарна крос-ентропія):\n",
        "\n",
        "Функція втрат крос-ентропії оцінює, наскільки добре модель передбачає класи, порівнюючи передбачені ймовірності $ \\hat{y} $ із справжніми мітками $ y $. Формула наступна:\n",
        "\n",
        "$$\n",
        "L(y, \\hat{y}) = - \\left[ y \\cdot \\log(\\hat{y}) + (1 - y) \\cdot \\log(1 - \\hat{y}) \\right]\n",
        "$$\n",
        "\n",
        "Де:\n",
        "- $ y $ — це справжнє значення (мітка класу, 0 або 1).\n",
        "- $ \\hat{y} $ — це передбачене значення (ймовірність).\n",
        "\n"
      ],
      "metadata": {
        "id": "lbLHTNfSclli"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.\n",
        "Тут вже наведений код для ініціювання набору даних в форматі numpy. Перетворіть `inputs`, `targets` на `torch` тензори. Виведіть результат на екран."
      ],
      "metadata": {
        "id": "GtOYB-RHfc_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "3BNXSR-VdYKQ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QLKZ77x4v_-v"
      },
      "outputs": [],
      "source": [
        "# Вхідні дані (temp, rainfall, humidity)\n",
        "inputs = np.array([[73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70]], dtype='float32')\n",
        "\n",
        "# Таргети (apples > 80)\n",
        "targets = np.array([[0],\n",
        "                    [1],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1]], dtype='float32')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.from_numpy(inputs)\n",
        "y = torch.from_numpy(targets)\n",
        "print(x)\n",
        "print(y)\n"
      ],
      "metadata": {
        "id": "KjoeaDrk6fO7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e663290-4701-4abc-90ed-f31f281600c1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 73.,  67.,  43.],\n",
            "        [ 91.,  88.,  64.],\n",
            "        [ 87., 134.,  58.],\n",
            "        [102.,  43.,  37.],\n",
            "        [ 69.,  96.,  70.]])\n",
            "tensor([[0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Ініціюйте ваги `w`, `b` для моделі логістичної регресії потрібної форми зважаючи на розмірності даних випадковими значеннями з нормального розподілу. Лишаю тут код для фіксації `random_seed`."
      ],
      "metadata": {
        "id": "iKzbJKfOgGV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.random.manual_seed(1)"
      ],
      "metadata": {
        "id": "aXhKw6Tdj1-d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fcd3521-657e-4564-84ae-6d7aafd6f4f8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7dc2d89f5c90>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w = torch.randn(1,3, requires_grad=True)\n",
        "b = torch.randn(1, requires_grad=True)\n",
        "print(w)\n",
        "print(b)"
      ],
      "metadata": {
        "id": "eApcB7eb6h9o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b92afead-cab5-423f-ce30-e80330d62833"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.6614, 0.2669, 0.0617]], requires_grad=True)\n",
            "tensor([0.6213], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Напишіть функцію `model`, яка буде обчислювати функцію гіпотези в логістичній регресії і дозволяти робити передбачення на основі введеного рядка даних і коефіцієнтів в змінних `w`, `b`.\n",
        "\n",
        "  **Важливий момент**, що функція `model` робить обчислення на `torch.tensors`, тож для математичних обчислень використовуємо фукнціонал `torch`, наприклад:\n",
        "  - обчсилення $e^x$: `torch.exp(x)`\n",
        "  - обчсилення $log(x)$: `torch.log(x)`\n",
        "  - обчислення середнього значення вектору `x`: `torch.mean(x)`\n",
        "\n",
        "  Використайте функцію `model` для обчислення передбачень з поточними значеннями `w`, `b`.Виведіть результат обчислень на екран.\n",
        "\n",
        "  Проаналізуйте передбачення. Чи не викликають вони у вас підозр? І якщо викликають, то чим це може бути зумовлено?"
      ],
      "metadata": {
        "id": "nYGxNGTaf5s6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model (x_param, w_param, b_param):\n",
        "  hypothesis_func = w_param @ x_param.T + b_param\n",
        "  logistic_func = 1/(1+torch.exp(-hypothesis_func))\n",
        "  return logistic_func, hypothesis_func"
      ],
      "metadata": {
        "id": "pSz2j4Fh6jBv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "probs, hypothesis_f = model(x_param=x, w_param = w, b_param = b)\n",
        "print(f'Probabilities results: {probs}, Hypothesis func: {hypothesis_f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-c00vrMnLHic",
        "outputId": "a2aa3ff1-8887-44e5-ab54-d0cc01c789f5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Probabilities results: tensor([[1., 1., 1., 1., 1.]], grad_fn=<MulBackward0>), Hypothesis func: tensor([[69.4361, 88.2410, 97.5041, 81.8390, 76.1967]], grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Усі результати передбачень дорівнюють одиниці, тобто модель на 100% вневнена і  віднесла всі записи до позитивного класу. Основною причиною такого передбачення може бути:\n",
        " - високі значення вагів для незалежних зміних. Як бачимо, значення функції гіпотези знаходяться в діапазоні від 60 до 100 , тому функція експоненти прямує до 1."
      ],
      "metadata": {
        "id": "q1IIbfh8P5dS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Напишіть функцію `binary_cross_entropy`, яка приймає на вхід передбачення моделі `predicted_probs` та справжні мітки в даних `true_labels` і обчислює значення втрат (loss)  за формулою бінарної крос-ентропії для кожного екземпляра та вертає середні втрати по всьому набору даних.\n",
        "  Використайте функцію `binary_cross_entropy` для обчислення втрат для поточних передбачень моделі."
      ],
      "metadata": {
        "id": "O2AGM0Mb2yHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "true_labels = y.view(-1)\n",
        "predicted_probs = probs.view(-1)\n",
        "\n",
        "def binary_cross_entropy(predicted_probs,true_labels):\n",
        "  loss = F.binary_cross_entropy(predicted_probs, true_labels)\n",
        "  return loss"
      ],
      "metadata": {
        "id": "xmK2kNiKqlVh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_results = binary_cross_entropy(predicted_probs=predicted_probs, true_labels=true_labels)\n",
        "print(f'Loss: {loss_results}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzdC0eUSzOhi",
        "outputId": "b1075003-07ac-4249-dd75-cced6fa66ac4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 40.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Зробіть зворотнє поширення помилки і виведіть градієнти за параметрами `w`, `b`. Проаналізуйте їх значення. Як гадаєте, чому вони саме такі?"
      ],
      "metadata": {
        "id": "ZFKpQxdHi1__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Обчислюємо градієнти\n",
        "loss_results.backward(retain_graph=True)"
      ],
      "metadata": {
        "id": "YAbXUNSJ6mCl"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(w)\n",
        "print(w.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adySlFoeGipn",
        "outputId": "c45922eb-4721-4756-bac2-0f61d3b6e404"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.6614, 0.2669, 0.0617]], requires_grad=True)\n",
            "tensor([[1.0201e-17, 9.3628e-18, 6.0090e-18]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(b)\n",
        "print(b.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nA_XRHGLfOmS",
        "outputId": "530e1cf2-925a-43b7-ce8d-f663d6186686"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.6213], requires_grad=True)\n",
            "tensor([1.3974e-19])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Дані значення градієнтів показують, що при незначному зменшенні наших вагів значення помилки не сильно зміниться - значення вагів в даному випадку підібрані невдало."
      ],
      "metadata": {
        "id": "isDNx-RP15Z0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Що сталось?**\n",
        "\n",
        "В цій задачі, коли ми ініціювали значення випадковими значеннями з нормального розподілу, насправді ці значення не були дуже гарними стартовими значеннями і привели до того, що градієнти стали дуже малими або навіть рівними нулю (це призводить до того, що градієнти \"зникають\"), і відповідно при оновленні ваг у нас не буде нічого змінюватись. Це називається `gradient vanishing`. Це відбувається через **насичення сигмоїдної функції активації.**\n",
        "\n",
        "У нашій задачі ми використовуємо сигмоїдну функцію активації, яка має такий вигляд:\n",
        "\n",
        "   $$\n",
        "   \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "   $$\n",
        "\n",
        "\n",
        "Коли значення $z$ дуже велике або дуже мале, сигмоїдна функція починає \"насичуватись\". Це означає, що для великих позитивних $z$ сигмоїда наближається до 1, а для великих негативних — до 0. В цих діапазонах градієнти починають стрімко зменшуватись і наближаються до нуля (бо градієнт - це похідна, похідна на проміжку функції, де вона паралельна осі ОХ, дорівнює 0), що робить оновлення ваг неможливим.\n",
        "\n",
        "![](https://editor.analyticsvidhya.com/uploads/27889vaegp.png)\n",
        "\n",
        "У логістичній регресії $ z = x \\cdot w + b $. Якщо ваги $w, b$ - великі, значення $z$ також буде великим, і сигмоїда перейде в насичену область, де градієнти дуже малі.\n",
        "\n",
        "Саме це сталося в нашій задачі, де великі випадкові значення ваг викликали насичення сигмоїдної функції. Це в свою чергу призводить до того, що під час зворотного поширення помилки (backpropagation) модель оновлює ваги дуже повільно або зовсім не оновлює. Це називається проблемою **зникнення градієнтів** (gradient vanishing problem).\n",
        "\n",
        "**Що ж робити?**\n",
        "Ініціювати ваги маленькими значеннями навколо нуля. Наприклад ми можемо просто в існуючій ініціалізації ваги розділити на 1000. Можна також використати інший спосіб ініціалізації вагів - інформація про це [тут](https://www.geeksforgeeks.org/initialize-weights-in-pytorch/).\n",
        "\n",
        "Як це робити - показую нижче. **Виконайте код та знову обчисліть передбачення, лосс і виведіть градієнти.**\n",
        "\n",
        "А я пишу пояснення, чому просто не зробити\n",
        "\n",
        "```\n",
        "w = torch.randn(1, 3, requires_grad=True)/1000\n",
        "b = torch.randn(1, requires_grad=True)/1000\n",
        "```\n",
        "\n",
        "Нам потрібно, аби тензори вагів були листовими (leaf tensors).\n",
        "\n",
        "1. **Що таке листовий тензор**\n",
        "Листовий тензор — це тензор, який був створений користувачем безпосередньо і з якого починається обчислювальний граф. Якщо такий тензор має `requires_grad=True`, PyTorch буде відслідковувати всі операції, виконані над ним, щоб правильно обчислювати градієнти під час навчання.\n",
        "\n",
        "2. **Чому ми використовуємо `w.data` замість звичайних операцій**\n",
        "Якщо ми просто виконали б операції, такі як `(w - 0.5) / 100`, ми б отримали **новий тензор**, який вже не був би листовим тензором, оскільки ці операції створюють **новий** тензор, а не модифікують існуючий.\n",
        "\n",
        "  Проте, щоб залишити наші тензори ваги `w` та зміщення `b` листовими і продовжити можливість відстеження градієнтів під час тренування, ми використовуємо атрибут `.data`. Цей атрибут дозволяє **виконувати операції in-place (прямо на існуючому тензорі)** без зміни самого об'єкта тензора. Отже, тензор залишається листовим, і PyTorch може коректно обчислювати його градієнти.\n",
        "\n",
        "3. **Чому важливо залишити тензор листовим**\n",
        "Якщо тензор більше не є листовим (наприклад, через проведення операцій, що створюють нові тензори), ви не зможете отримати градієнти за допомогою `w.grad` чи `b.grad` після виклику `loss.backward()`. Це може призвести до втрати можливості оновлення параметрів під час тренування моделі. В нашому випадку ми хочемо, щоб тензори `w` та `b` накопичували градієнти, тому вони повинні залишатись листовими.\n",
        "\n",
        "**Висновок:**\n",
        "Ми використовуємо `.data`, щоб виконати операції зміни значень на ваги і зміщення **in-place**, залишаючи їх листовими тензорами, які можуть накопичувати градієнти під час навчання. Це дозволяє коректно працювати механізму зворотного поширення помилки (backpropagation) і оновлювати ваги моделі."
      ],
      "metadata": {
        "id": "nDN1t1RujQsK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Виконайте код та знову обчисліть передбачення, лосс і знайдіть градієнти та виведіть всі ці тензори на екран."
      ],
      "metadata": {
        "id": "rOPSQyttpVjO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.random.manual_seed(1)\n",
        "w = torch.randn(1, 3, requires_grad=True)  # Листовий тензор\n",
        "b = torch.randn(1, requires_grad=True)     # Листовий тензор\n",
        "\n",
        "# in-place операції\n",
        "w.data = w.data / 1000\n",
        "b.data = b.data / 1000"
      ],
      "metadata": {
        "id": "-EBOJ3tsnRaD"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "probs_2, hypothesis_2 = model(x_param=x, w_param = w.data, b_param = b.data)\n",
        "print(f'Probabilities results: {probs_2}, Hypothesis func: {hypothesis_2}')"
      ],
      "metadata": {
        "id": "-JwXiSpX6orh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d14082a5-18fd-4a96-b386-06b607032244"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Probabilities results: tensor([[0.5174, 0.5220, 0.5244, 0.5204, 0.5190]]), Hypothesis func: tensor([[0.0694, 0.0882, 0.0975, 0.0818, 0.0762]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_results_2 = binary_cross_entropy(predicted_probs=probs_2.view(-1), true_labels=y.view(-1))\n",
        "print(f'Loss: {loss_results_2}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CwBDKTr_96PG",
        "outputId": "653272e9-a4ba-48a7-8af2-69742c458cd3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.6829456686973572\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Модель вже не 100% вневнена і значення помилки значно зменшилось."
      ],
      "metadata": {
        "id": "2B_33B0b-Tyc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Напишіть алгоритм градієнтного спуску, який буде навчати модель з використанням написаних раніше функцій і виконуючи оновлення ваг. Алгоритм має включати наступні кроки:\n",
        "\n",
        "  1. Генерація прогнозів\n",
        "  2. Обчислення втрат\n",
        "  3. Обчислення градієнтів (gradients) loss-фукнції відносно ваг і зсувів\n",
        "  4. Налаштування ваг шляхом віднімання невеликої величини, пропорційної градієнту (`learning_rate` домножений на градієнт)\n",
        "  5. Скидання градієнтів на нуль\n",
        "\n",
        "Виконайте градієнтний спуск протягом 1000 епох, обчисліть фінальні передбачення і проаналізуйте, чи вони точні?"
      ],
      "metadata": {
        "id": "RCdi44IT334o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(100):\n",
        "    preds_proba, hypot_func = model(x_param=x, w_param=w, b_param=b)\n",
        "    loss = binary_cross_entropy(predicted_probs=preds_proba.view(-1),true_labels=y.view(-1))\n",
        "    print(f'Loss for {i}: {loss}')\n",
        "    loss.backward()\n",
        "    with torch.no_grad():\n",
        "        w -= w.grad * 1e-5\n",
        "        b -= b.grad * 1e-5\n",
        "        print(f'W for {i}: {w}')\n",
        "        print(f'B for {i}: {b}')\n",
        "        w.grad.zero_()\n",
        "        b.grad.zero_()"
      ],
      "metadata": {
        "id": "vni6qV3Ft1ms",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "ae89623c-0bd2-46ff-8982-b6512a24aa9e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss for 0: 0.5625805258750916\n",
            "W for 0: tensor([[-0.0031,  0.0086,  0.0037]], requires_grad=True)\n",
            "B for 0: tensor([0.0006], requires_grad=True)\n",
            "Loss for 1: 0.5619428753852844\n",
            "W for 1: tensor([[-0.0032,  0.0087,  0.0037]], requires_grad=True)\n",
            "B for 1: tensor([0.0006], requires_grad=True)\n",
            "Loss for 2: 0.5613070726394653\n",
            "W for 2: tensor([[-0.0032,  0.0087,  0.0038]], requires_grad=True)\n",
            "B for 2: tensor([0.0006], requires_grad=True)\n",
            "Loss for 3: 0.5606730580329895\n",
            "W for 3: tensor([[-0.0033,  0.0088,  0.0038]], requires_grad=True)\n",
            "B for 3: tensor([0.0006], requires_grad=True)\n",
            "Loss for 4: 0.5600409507751465\n",
            "W for 4: tensor([[-0.0033,  0.0088,  0.0038]], requires_grad=True)\n",
            "B for 4: tensor([0.0006], requires_grad=True)\n",
            "Loss for 5: 0.5594106912612915\n",
            "W for 5: tensor([[-0.0034,  0.0089,  0.0038]], requires_grad=True)\n",
            "B for 5: tensor([0.0006], requires_grad=True)\n",
            "Loss for 6: 0.5587823987007141\n",
            "W for 6: tensor([[-0.0034,  0.0089,  0.0038]], requires_grad=True)\n",
            "B for 6: tensor([0.0006], requires_grad=True)\n",
            "Loss for 7: 0.55815589427948\n",
            "W for 7: tensor([[-0.0035,  0.0090,  0.0038]], requires_grad=True)\n",
            "B for 7: tensor([0.0006], requires_grad=True)\n",
            "Loss for 8: 0.5575310587882996\n",
            "W for 8: tensor([[-0.0036,  0.0090,  0.0039]], requires_grad=True)\n",
            "B for 8: tensor([0.0006], requires_grad=True)\n",
            "Loss for 9: 0.5569082498550415\n",
            "W for 9: tensor([[-0.0036,  0.0091,  0.0039]], requires_grad=True)\n",
            "B for 9: tensor([0.0006], requires_grad=True)\n",
            "Loss for 10: 0.5562872290611267\n",
            "W for 10: tensor([[-0.0037,  0.0091,  0.0039]], requires_grad=True)\n",
            "B for 10: tensor([0.0006], requires_grad=True)\n",
            "Loss for 11: 0.5556679368019104\n",
            "W for 11: tensor([[-0.0037,  0.0092,  0.0039]], requires_grad=True)\n",
            "B for 11: tensor([0.0006], requires_grad=True)\n",
            "Loss for 12: 0.5550504922866821\n",
            "W for 12: tensor([[-0.0038,  0.0092,  0.0039]], requires_grad=True)\n",
            "B for 12: tensor([0.0006], requires_grad=True)\n",
            "Loss for 13: 0.5544347167015076\n",
            "W for 13: tensor([[-0.0039,  0.0093,  0.0039]], requires_grad=True)\n",
            "B for 13: tensor([0.0006], requires_grad=True)\n",
            "Loss for 14: 0.5538208484649658\n",
            "W for 14: tensor([[-0.0039,  0.0093,  0.0040]], requires_grad=True)\n",
            "B for 14: tensor([0.0006], requires_grad=True)\n",
            "Loss for 15: 0.5532087087631226\n",
            "W for 15: tensor([[-0.0040,  0.0094,  0.0040]], requires_grad=True)\n",
            "B for 15: tensor([0.0006], requires_grad=True)\n",
            "Loss for 16: 0.5525983572006226\n",
            "W for 16: tensor([[-0.0040,  0.0094,  0.0040]], requires_grad=True)\n",
            "B for 16: tensor([0.0006], requires_grad=True)\n",
            "Loss for 17: 0.551989734172821\n",
            "W for 17: tensor([[-0.0041,  0.0094,  0.0040]], requires_grad=True)\n",
            "B for 17: tensor([0.0006], requires_grad=True)\n",
            "Loss for 18: 0.5513829588890076\n",
            "W for 18: tensor([[-0.0042,  0.0095,  0.0040]], requires_grad=True)\n",
            "B for 18: tensor([0.0006], requires_grad=True)\n",
            "Loss for 19: 0.5507778525352478\n",
            "W for 19: tensor([[-0.0042,  0.0095,  0.0040]], requires_grad=True)\n",
            "B for 19: tensor([0.0006], requires_grad=True)\n",
            "Loss for 20: 0.5501744747161865\n",
            "W for 20: tensor([[-0.0043,  0.0096,  0.0041]], requires_grad=True)\n",
            "B for 20: tensor([0.0006], requires_grad=True)\n",
            "Loss for 21: 0.5495728254318237\n",
            "W for 21: tensor([[-0.0043,  0.0096,  0.0041]], requires_grad=True)\n",
            "B for 21: tensor([0.0006], requires_grad=True)\n",
            "Loss for 22: 0.5489729046821594\n",
            "W for 22: tensor([[-0.0044,  0.0097,  0.0041]], requires_grad=True)\n",
            "B for 22: tensor([0.0006], requires_grad=True)\n",
            "Loss for 23: 0.5483747720718384\n",
            "W for 23: tensor([[-0.0044,  0.0097,  0.0041]], requires_grad=True)\n",
            "B for 23: tensor([0.0006], requires_grad=True)\n",
            "Loss for 24: 0.5477782487869263\n",
            "W for 24: tensor([[-0.0045,  0.0098,  0.0041]], requires_grad=True)\n",
            "B for 24: tensor([0.0006], requires_grad=True)\n",
            "Loss for 25: 0.5471835136413574\n",
            "W for 25: tensor([[-0.0046,  0.0098,  0.0042]], requires_grad=True)\n",
            "B for 25: tensor([0.0006], requires_grad=True)\n",
            "Loss for 26: 0.5465905070304871\n",
            "W for 26: tensor([[-0.0046,  0.0099,  0.0042]], requires_grad=True)\n",
            "B for 26: tensor([0.0006], requires_grad=True)\n",
            "Loss for 27: 0.5459991693496704\n",
            "W for 27: tensor([[-0.0047,  0.0099,  0.0042]], requires_grad=True)\n",
            "B for 27: tensor([0.0006], requires_grad=True)\n",
            "Loss for 28: 0.5454094409942627\n",
            "W for 28: tensor([[-0.0047,  0.0100,  0.0042]], requires_grad=True)\n",
            "B for 28: tensor([0.0006], requires_grad=True)\n",
            "Loss for 29: 0.5448214411735535\n",
            "W for 29: tensor([[-0.0048,  0.0100,  0.0042]], requires_grad=True)\n",
            "B for 29: tensor([0.0006], requires_grad=True)\n",
            "Loss for 30: 0.544235110282898\n",
            "W for 30: tensor([[-0.0049,  0.0101,  0.0042]], requires_grad=True)\n",
            "B for 30: tensor([0.0006], requires_grad=True)\n",
            "Loss for 31: 0.5436505079269409\n",
            "W for 31: tensor([[-0.0049,  0.0101,  0.0043]], requires_grad=True)\n",
            "B for 31: tensor([0.0006], requires_grad=True)\n",
            "Loss for 32: 0.5430675745010376\n",
            "W for 32: tensor([[-0.0050,  0.0102,  0.0043]], requires_grad=True)\n",
            "B for 32: tensor([0.0006], requires_grad=True)\n",
            "Loss for 33: 0.5424862504005432\n",
            "W for 33: tensor([[-0.0050,  0.0102,  0.0043]], requires_grad=True)\n",
            "B for 33: tensor([0.0006], requires_grad=True)\n",
            "Loss for 34: 0.5419066548347473\n",
            "W for 34: tensor([[-0.0051,  0.0103,  0.0043]], requires_grad=True)\n",
            "B for 34: tensor([0.0006], requires_grad=True)\n",
            "Loss for 35: 0.5413286089897156\n",
            "W for 35: tensor([[-0.0051,  0.0103,  0.0043]], requires_grad=True)\n",
            "B for 35: tensor([0.0006], requires_grad=True)\n",
            "Loss for 36: 0.5407522916793823\n",
            "W for 36: tensor([[-0.0052,  0.0103,  0.0043]], requires_grad=True)\n",
            "B for 36: tensor([0.0006], requires_grad=True)\n",
            "Loss for 37: 0.5401775240898132\n",
            "W for 37: tensor([[-0.0053,  0.0104,  0.0044]], requires_grad=True)\n",
            "B for 37: tensor([0.0006], requires_grad=True)\n",
            "Loss for 38: 0.5396044850349426\n",
            "W for 38: tensor([[-0.0053,  0.0104,  0.0044]], requires_grad=True)\n",
            "B for 38: tensor([0.0006], requires_grad=True)\n",
            "Loss for 39: 0.539033055305481\n",
            "W for 39: tensor([[-0.0054,  0.0105,  0.0044]], requires_grad=True)\n",
            "B for 39: tensor([0.0006], requires_grad=True)\n",
            "Loss for 40: 0.5384632349014282\n",
            "W for 40: tensor([[-0.0054,  0.0105,  0.0044]], requires_grad=True)\n",
            "B for 40: tensor([0.0006], requires_grad=True)\n",
            "Loss for 41: 0.5378949642181396\n",
            "W for 41: tensor([[-0.0055,  0.0106,  0.0044]], requires_grad=True)\n",
            "B for 41: tensor([0.0006], requires_grad=True)\n",
            "Loss for 42: 0.5373283624649048\n",
            "W for 42: tensor([[-0.0055,  0.0106,  0.0044]], requires_grad=True)\n",
            "B for 42: tensor([0.0006], requires_grad=True)\n",
            "Loss for 43: 0.5367634892463684\n",
            "W for 43: tensor([[-0.0056,  0.0107,  0.0045]], requires_grad=True)\n",
            "B for 43: tensor([0.0006], requires_grad=True)\n",
            "Loss for 44: 0.5362001061439514\n",
            "W for 44: tensor([[-0.0057,  0.0107,  0.0045]], requires_grad=True)\n",
            "B for 44: tensor([0.0006], requires_grad=True)\n",
            "Loss for 45: 0.5356383323669434\n",
            "W for 45: tensor([[-0.0057,  0.0108,  0.0045]], requires_grad=True)\n",
            "B for 45: tensor([0.0006], requires_grad=True)\n",
            "Loss for 46: 0.5350781679153442\n",
            "W for 46: tensor([[-0.0058,  0.0108,  0.0045]], requires_grad=True)\n",
            "B for 46: tensor([0.0006], requires_grad=True)\n",
            "Loss for 47: 0.5345195531845093\n",
            "W for 47: tensor([[-0.0058,  0.0109,  0.0045]], requires_grad=True)\n",
            "B for 47: tensor([0.0006], requires_grad=True)\n",
            "Loss for 48: 0.5339624881744385\n",
            "W for 48: tensor([[-0.0059,  0.0109,  0.0045]], requires_grad=True)\n",
            "B for 48: tensor([0.0006], requires_grad=True)\n",
            "Loss for 49: 0.5334070920944214\n",
            "W for 49: tensor([[-0.0059,  0.0109,  0.0045]], requires_grad=True)\n",
            "B for 49: tensor([0.0006], requires_grad=True)\n",
            "Loss for 50: 0.5328532457351685\n",
            "W for 50: tensor([[-0.0060,  0.0110,  0.0046]], requires_grad=True)\n",
            "B for 50: tensor([0.0006], requires_grad=True)\n",
            "Loss for 51: 0.5323009490966797\n",
            "W for 51: tensor([[-0.0061,  0.0110,  0.0046]], requires_grad=True)\n",
            "B for 51: tensor([0.0006], requires_grad=True)\n",
            "Loss for 52: 0.5317502021789551\n",
            "W for 52: tensor([[-0.0061,  0.0111,  0.0046]], requires_grad=True)\n",
            "B for 52: tensor([0.0006], requires_grad=True)\n",
            "Loss for 53: 0.5312010645866394\n",
            "W for 53: tensor([[-0.0062,  0.0111,  0.0046]], requires_grad=True)\n",
            "B for 53: tensor([0.0006], requires_grad=True)\n",
            "Loss for 54: 0.5306534767150879\n",
            "W for 54: tensor([[-0.0062,  0.0112,  0.0046]], requires_grad=True)\n",
            "B for 54: tensor([0.0006], requires_grad=True)\n",
            "Loss for 55: 0.5301073789596558\n",
            "W for 55: tensor([[-0.0063,  0.0112,  0.0046]], requires_grad=True)\n",
            "B for 55: tensor([0.0006], requires_grad=True)\n",
            "Loss for 56: 0.5295628309249878\n",
            "W for 56: tensor([[-0.0063,  0.0113,  0.0047]], requires_grad=True)\n",
            "B for 56: tensor([0.0006], requires_grad=True)\n",
            "Loss for 57: 0.5290198922157288\n",
            "W for 57: tensor([[-0.0064,  0.0113,  0.0047]], requires_grad=True)\n",
            "B for 57: tensor([0.0006], requires_grad=True)\n",
            "Loss for 58: 0.5284784436225891\n",
            "W for 58: tensor([[-0.0064,  0.0114,  0.0047]], requires_grad=True)\n",
            "B for 58: tensor([0.0006], requires_grad=True)\n",
            "Loss for 59: 0.5279384851455688\n",
            "W for 59: tensor([[-0.0065,  0.0114,  0.0047]], requires_grad=True)\n",
            "B for 59: tensor([0.0006], requires_grad=True)\n",
            "Loss for 60: 0.5274001359939575\n",
            "W for 60: tensor([[-0.0066,  0.0114,  0.0047]], requires_grad=True)\n",
            "B for 60: tensor([0.0006], requires_grad=True)\n",
            "Loss for 61: 0.5268632173538208\n",
            "W for 61: tensor([[-0.0066,  0.0115,  0.0047]], requires_grad=True)\n",
            "B for 61: tensor([0.0006], requires_grad=True)\n",
            "Loss for 62: 0.526327908039093\n",
            "W for 62: tensor([[-0.0067,  0.0115,  0.0048]], requires_grad=True)\n",
            "B for 62: tensor([0.0006], requires_grad=True)\n",
            "Loss for 63: 0.5257940292358398\n",
            "W for 63: tensor([[-0.0067,  0.0116,  0.0048]], requires_grad=True)\n",
            "B for 63: tensor([0.0006], requires_grad=True)\n",
            "Loss for 64: 0.525261640548706\n",
            "W for 64: tensor([[-0.0068,  0.0116,  0.0048]], requires_grad=True)\n",
            "B for 64: tensor([0.0006], requires_grad=True)\n",
            "Loss for 65: 0.5247308015823364\n",
            "W for 65: tensor([[-0.0068,  0.0117,  0.0048]], requires_grad=True)\n",
            "B for 65: tensor([0.0006], requires_grad=True)\n",
            "Loss for 66: 0.524201512336731\n",
            "W for 66: tensor([[-0.0069,  0.0117,  0.0048]], requires_grad=True)\n",
            "B for 66: tensor([0.0006], requires_grad=True)\n",
            "Loss for 67: 0.5236736536026001\n",
            "W for 67: tensor([[-0.0069,  0.0118,  0.0048]], requires_grad=True)\n",
            "B for 67: tensor([0.0005], requires_grad=True)\n",
            "Loss for 68: 0.5231472253799438\n",
            "W for 68: tensor([[-0.0070,  0.0118,  0.0049]], requires_grad=True)\n",
            "B for 68: tensor([0.0005], requires_grad=True)\n",
            "Loss for 69: 0.5226224660873413\n",
            "W for 69: tensor([[-0.0071,  0.0118,  0.0049]], requires_grad=True)\n",
            "B for 69: tensor([0.0005], requires_grad=True)\n",
            "Loss for 70: 0.5220988988876343\n",
            "W for 70: tensor([[-0.0071,  0.0119,  0.0049]], requires_grad=True)\n",
            "B for 70: tensor([0.0005], requires_grad=True)\n",
            "Loss for 71: 0.521577000617981\n",
            "W for 71: tensor([[-0.0072,  0.0119,  0.0049]], requires_grad=True)\n",
            "B for 71: tensor([0.0005], requires_grad=True)\n",
            "Loss for 72: 0.5210564732551575\n",
            "W for 72: tensor([[-0.0072,  0.0120,  0.0049]], requires_grad=True)\n",
            "B for 72: tensor([0.0005], requires_grad=True)\n",
            "Loss for 73: 0.5205374956130981\n",
            "W for 73: tensor([[-0.0073,  0.0120,  0.0049]], requires_grad=True)\n",
            "B for 73: tensor([0.0005], requires_grad=True)\n",
            "Loss for 74: 0.5200200080871582\n",
            "W for 74: tensor([[-0.0073,  0.0121,  0.0050]], requires_grad=True)\n",
            "B for 74: tensor([0.0005], requires_grad=True)\n",
            "Loss for 75: 0.5195038914680481\n",
            "W for 75: tensor([[-0.0074,  0.0121,  0.0050]], requires_grad=True)\n",
            "B for 75: tensor([0.0005], requires_grad=True)\n",
            "Loss for 76: 0.5189892053604126\n",
            "W for 76: tensor([[-0.0074,  0.0121,  0.0050]], requires_grad=True)\n",
            "B for 76: tensor([0.0005], requires_grad=True)\n",
            "Loss for 77: 0.5184759497642517\n",
            "W for 77: tensor([[-0.0075,  0.0122,  0.0050]], requires_grad=True)\n",
            "B for 77: tensor([0.0005], requires_grad=True)\n",
            "Loss for 78: 0.517964243888855\n",
            "W for 78: tensor([[-0.0076,  0.0122,  0.0050]], requires_grad=True)\n",
            "B for 78: tensor([0.0005], requires_grad=True)\n",
            "Loss for 79: 0.5174537897109985\n",
            "W for 79: tensor([[-0.0076,  0.0123,  0.0050]], requires_grad=True)\n",
            "B for 79: tensor([0.0005], requires_grad=True)\n",
            "Loss for 80: 0.5169450044631958\n",
            "W for 80: tensor([[-0.0077,  0.0123,  0.0050]], requires_grad=True)\n",
            "B for 80: tensor([0.0005], requires_grad=True)\n",
            "Loss for 81: 0.5164374113082886\n",
            "W for 81: tensor([[-0.0077,  0.0124,  0.0051]], requires_grad=True)\n",
            "B for 81: tensor([0.0005], requires_grad=True)\n",
            "Loss for 82: 0.5159314274787903\n",
            "W for 82: tensor([[-0.0078,  0.0124,  0.0051]], requires_grad=True)\n",
            "B for 82: tensor([0.0005], requires_grad=True)\n",
            "Loss for 83: 0.515426754951477\n",
            "W for 83: tensor([[-0.0078,  0.0124,  0.0051]], requires_grad=True)\n",
            "B for 83: tensor([0.0005], requires_grad=True)\n",
            "Loss for 84: 0.5149235725402832\n",
            "W for 84: tensor([[-0.0079,  0.0125,  0.0051]], requires_grad=True)\n",
            "B for 84: tensor([0.0005], requires_grad=True)\n",
            "Loss for 85: 0.5144217014312744\n",
            "W for 85: tensor([[-0.0079,  0.0125,  0.0051]], requires_grad=True)\n",
            "B for 85: tensor([0.0005], requires_grad=True)\n",
            "Loss for 86: 0.5139212608337402\n",
            "W for 86: tensor([[-0.0080,  0.0126,  0.0051]], requires_grad=True)\n",
            "B for 86: tensor([0.0005], requires_grad=True)\n",
            "Loss for 87: 0.5134222507476807\n",
            "W for 87: tensor([[-0.0080,  0.0126,  0.0052]], requires_grad=True)\n",
            "B for 87: tensor([0.0005], requires_grad=True)\n",
            "Loss for 88: 0.5129246115684509\n",
            "W for 88: tensor([[-0.0081,  0.0127,  0.0052]], requires_grad=True)\n",
            "B for 88: tensor([0.0005], requires_grad=True)\n",
            "Loss for 89: 0.5124284029006958\n",
            "W for 89: tensor([[-0.0081,  0.0127,  0.0052]], requires_grad=True)\n",
            "B for 89: tensor([0.0005], requires_grad=True)\n",
            "Loss for 90: 0.511933445930481\n",
            "W for 90: tensor([[-0.0082,  0.0127,  0.0052]], requires_grad=True)\n",
            "B for 90: tensor([0.0005], requires_grad=True)\n",
            "Loss for 91: 0.5114399790763855\n",
            "W for 91: tensor([[-0.0083,  0.0128,  0.0052]], requires_grad=True)\n",
            "B for 91: tensor([0.0005], requires_grad=True)\n",
            "Loss for 92: 0.5109478831291199\n",
            "W for 92: tensor([[-0.0083,  0.0128,  0.0052]], requires_grad=True)\n",
            "B for 92: tensor([0.0005], requires_grad=True)\n",
            "Loss for 93: 0.5104572176933289\n",
            "W for 93: tensor([[-0.0084,  0.0129,  0.0053]], requires_grad=True)\n",
            "B for 93: tensor([0.0005], requires_grad=True)\n",
            "Loss for 94: 0.5099678635597229\n",
            "W for 94: tensor([[-0.0084,  0.0129,  0.0053]], requires_grad=True)\n",
            "B for 94: tensor([0.0005], requires_grad=True)\n",
            "Loss for 95: 0.509479820728302\n",
            "W for 95: tensor([[-0.0085,  0.0130,  0.0053]], requires_grad=True)\n",
            "B for 95: tensor([0.0005], requires_grad=True)\n",
            "Loss for 96: 0.5089932680130005\n",
            "W for 96: tensor([[-0.0085,  0.0130,  0.0053]], requires_grad=True)\n",
            "B for 96: tensor([0.0005], requires_grad=True)\n",
            "Loss for 97: 0.5085079073905945\n",
            "W for 97: tensor([[-0.0086,  0.0130,  0.0053]], requires_grad=True)\n",
            "B for 97: tensor([0.0005], requires_grad=True)\n",
            "Loss for 98: 0.5080239176750183\n",
            "W for 98: tensor([[-0.0086,  0.0131,  0.0053]], requires_grad=True)\n",
            "B for 98: tensor([0.0005], requires_grad=True)\n",
            "Loss for 99: 0.507541298866272\n",
            "W for 99: tensor([[-0.0087,  0.0131,  0.0053]], requires_grad=True)\n",
            "B for 99: tensor([0.0005], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds_proba, hypot_func = model(x_param=x, w_param=w, b_param=b)\n",
        "preds_proba"
      ],
      "metadata": {
        "id": "vx-jM9LevQxW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "645285dc-2fde-4dad-82f6-e8db48ad0a16"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.6169, 0.6700, 0.7883, 0.4695, 0.7382]], grad_fn=<MulBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "targets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3aEFd60IeWYh",
        "outputId": "531cc9fa-bc3f-459e-80b9-09e84e923dc9"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = binary_cross_entropy(predicted_probs=preds_proba.view(-1),true_labels=y.view(-1))\n",
        "loss"
      ],
      "metadata": {
        "id": "e-qw3089xesf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39d03f83-5621-4602-b0f2-7cdf21666c69"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.5071, grad_fn=<BinaryCrossEntropyBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jcX_NI_evdWi",
        "outputId": "f07ef12d-7404-4dbe-f7ce-ecea7aba54a2"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.0087,  0.0131,  0.0053]], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1kJsTJwPvgF-",
        "outputId": "e72c9d9f-e122-4412-ee9c-94d24cd028c6"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0005], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Значення помилки при кожній ітерації зменшується. Також при порівнянні прогнозованих передбачень та цільових змінних, модель  в цілому непогано передбачила позитивний клас."
      ],
      "metadata": {
        "id": "7zokuMvra9zf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Секція 2. Створення лог регресії з використанням функціоналу `torch.nn`.**\n",
        "\n",
        "Давайте повторно реалізуємо ту ж модель, використовуючи деякі вбудовані функції та класи з PyTorch.\n",
        "\n",
        "Даних у нас буде побільше - тож, визначаємо нові масиви."
      ],
      "metadata": {
        "id": "fuRhlyF9qAia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Вхідні дані (temp, rainfall, humidity)\n",
        "inputs = np.array([[73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70],\n",
        "                   [73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70],\n",
        "                   [73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70]], dtype='float32')\n",
        "\n",
        "# Таргети (apples > 80)\n",
        "targets = np.array([[0],\n",
        "                    [1],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1]], dtype='float32')"
      ],
      "metadata": {
        "id": "IX8Bhm74rV4M"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Завантажте вхідні дані та мітки в PyTorch тензори та з них створіть датасет, який поєднує вхідні дані з мітками, використовуючи клас `TensorDataset`. Виведіть перші 3 елементи в датасеті.\n",
        "\n"
      ],
      "metadata": {
        "id": "7X2dV30KtAPu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader"
      ],
      "metadata": {
        "id": "chrvMfBs6vjo"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = torch.from_numpy(inputs)\n",
        "targets = torch.from_numpy(targets)\n",
        "train_ds = TensorDataset(inputs, targets)\n",
        "train_ds[0:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTeS2Y519Hkg",
        "outputId": "ef5e5d49-a48d-4301-8635-b4eebc5222eb"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 73.,  67.,  43.],\n",
              "         [ 91.,  88.,  64.],\n",
              "         [ 87., 134.,  58.]]),\n",
              " tensor([[0.],\n",
              "         [1.],\n",
              "         [1.]]))"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Визначте data loader з класом **DataLoader** для підготовленого датасету `train_ds`, встановіть розмір батчу на 5 та увімкніть перемішування даних для ефективного навчання моделі. Виведіть перший елемент в дата лоадері."
      ],
      "metadata": {
        "id": "4nMFaa8suOd3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 5\n",
        "train_dl = DataLoader(train_ds, batch_size, shuffle=True)\n",
        "next(iter(train_dl))"
      ],
      "metadata": {
        "id": "ZCsRo5Mx6wEI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33d33218-7dde-4819-db2c-11d58529ed8b"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([[ 73.,  67.,  43.],\n",
              "         [ 91.,  88.,  64.],\n",
              "         [102.,  43.,  37.],\n",
              "         [ 69.,  96.,  70.],\n",
              "         [102.,  43.,  37.]]),\n",
              " tensor([[0.],\n",
              "         [1.],\n",
              "         [0.],\n",
              "         [1.],\n",
              "         [0.]])]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Створіть клас `LogReg` для логістичної регресії, наслідуючи модуль `torch.nn.Module` за прикладом в лекції (в частині про FeedForward мережі).\n",
        "\n",
        "  У нас модель складається з лінійної комбінації вхідних значень і застосування фукнції сигмоїда. Тож, нейромережа буде складатись з лінійного шару `nn.Linear` і використання активації `nn.Sigmid`. У створеному класі мають бути реалізовані методи `__init__` з ініціалізацією шарів і метод `forward` для виконання прямого проходу моделі через лінійний шар і функцію активації.\n",
        "\n",
        "  Створіть екземпляр класу `LogReg` в змінній `model`."
      ],
      "metadata": {
        "id": "ymcQOo_hum6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class LogReg(nn.Module):\n",
        "    # Initialize the layers\n",
        "    def __init__(self, input_dim):\n",
        "        super(LogReg, self).__init__()\n",
        "        self.linear = nn.Linear(input_dim, 1)\n",
        "        self.act = nn.Sigmoid() # Activation function\n",
        "\n",
        "    # Perform the computation\n",
        "    def forward(self, x):\n",
        "        x = self.linear(x)\n",
        "        x = self.act(x)\n",
        "        return x\n",
        "\n",
        "input_dim = 3\n",
        "model = LogReg(input_dim)"
      ],
      "metadata": {
        "id": "EyAwhTBW6xxz"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Задайте оптимізатор `Stockastic Gradient Descent` в змінній `opt` для навчання моделі логістичної регресії. А також визначіть в змінній `loss` функцію втрат `binary_cross_entropy` з модуля `torch.nn.functional` для обчислення втрат моделі. Обчисліть втрати для поточних передбачень і міток, а потім виведіть їх. Зробіть висновок, чи моделі вдалось навчитись?"
      ],
      "metadata": {
        "id": "RflV7xeVyoJy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "opt = torch.optim.SGD(model.parameters(), lr=1e-5)\n",
        "loss_fn = F.binary_cross_entropy\n",
        "loss_result = loss_fn(model(inputs), targets)\n",
        "loss_result"
      ],
      "metadata": {
        "id": "3QCATPU_6yfa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "344a1dd4-c32c-4f4a-bc18-41bb86fb6bbe"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(7.6312, grad_fn=<BinaryCrossEntropyBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Значення помилки є високим. Ініціалізація початкових вагів є поки що невдалою, моделі поки не вдалось навчитись."
      ],
      "metadata": {
        "id": "M51vUhiDjZ3_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Візьміть з лекції функцію для тренування моделі з відстеженням значень втрат і навчіть щойно визначену модель на 1000 епохах. Виведіть після цього графік зміни loss, фінальні передбачення і значення таргетів."
      ],
      "metadata": {
        "id": "ch-WrYnKzMzq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fit_return_loss(num_epochs, model, loss_fn, opt, train_dl):\n",
        "    losses = []\n",
        "    for epoch in range(num_epochs):\n",
        "        # Ініціалізуємо акумулятор для втрат\n",
        "        total_loss = 0\n",
        "\n",
        "        for xb, yb in train_dl:\n",
        "            # Генеруємо передбачення\n",
        "            pred = model(xb)\n",
        "\n",
        "            # Обчислюємо втрати\n",
        "            loss = loss_fn(pred, yb)\n",
        "\n",
        "            # Виконуємо градієнтний спуск\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            opt.zero_grad()\n",
        "\n",
        "            # Накопичуємо втрати\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # Обчислюємо середні втрати для епохи\n",
        "        avg_loss = total_loss / len(train_dl)\n",
        "        losses.append(avg_loss)\n",
        "\n",
        "        # Виводимо підсумок епохи\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "          print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n",
        "    return losses"
      ],
      "metadata": {
        "id": "cEHQH9qE626k"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = fit_return_loss(1000, model, loss_fn, opt, train_dl)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sb3QMzAqrwHx",
        "outputId": "f30f42df-e573-4e6a-8344-5cf7d3958203"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/1000], Loss: 6.6400\n",
            "Epoch [20/1000], Loss: 6.0483\n",
            "Epoch [30/1000], Loss: 5.7263\n",
            "Epoch [40/1000], Loss: 5.4710\n",
            "Epoch [50/1000], Loss: 5.2474\n",
            "Epoch [60/1000], Loss: 5.0653\n",
            "Epoch [70/1000], Loss: 4.8974\n",
            "Epoch [80/1000], Loss: 4.7861\n",
            "Epoch [90/1000], Loss: 4.6903\n",
            "Epoch [100/1000], Loss: 4.5353\n",
            "Epoch [110/1000], Loss: 4.4753\n",
            "Epoch [120/1000], Loss: 4.3686\n",
            "Epoch [130/1000], Loss: 4.2792\n",
            "Epoch [140/1000], Loss: 4.2018\n",
            "Epoch [150/1000], Loss: 4.1132\n",
            "Epoch [160/1000], Loss: 4.0389\n",
            "Epoch [170/1000], Loss: 3.9423\n",
            "Epoch [180/1000], Loss: 3.8701\n",
            "Epoch [190/1000], Loss: 3.7756\n",
            "Epoch [200/1000], Loss: 3.6974\n",
            "Epoch [210/1000], Loss: 3.6101\n",
            "Epoch [220/1000], Loss: 3.5297\n",
            "Epoch [230/1000], Loss: 3.4427\n",
            "Epoch [240/1000], Loss: 3.3760\n",
            "Epoch [250/1000], Loss: 3.2822\n",
            "Epoch [260/1000], Loss: 3.1985\n",
            "Epoch [270/1000], Loss: 3.1111\n",
            "Epoch [280/1000], Loss: 3.0308\n",
            "Epoch [290/1000], Loss: 2.9483\n",
            "Epoch [300/1000], Loss: 2.8748\n",
            "Epoch [310/1000], Loss: 2.7879\n",
            "Epoch [320/1000], Loss: 2.7048\n",
            "Epoch [330/1000], Loss: 2.6292\n",
            "Epoch [340/1000], Loss: 2.5456\n",
            "Epoch [350/1000], Loss: 2.4710\n",
            "Epoch [360/1000], Loss: 2.4122\n",
            "Epoch [370/1000], Loss: 2.3102\n",
            "Epoch [380/1000], Loss: 2.2363\n",
            "Epoch [390/1000], Loss: 2.1587\n",
            "Epoch [400/1000], Loss: 2.0854\n",
            "Epoch [410/1000], Loss: 2.0062\n",
            "Epoch [420/1000], Loss: 1.9342\n",
            "Epoch [430/1000], Loss: 1.8632\n",
            "Epoch [440/1000], Loss: 1.7922\n",
            "Epoch [450/1000], Loss: 1.7232\n",
            "Epoch [460/1000], Loss: 1.6485\n",
            "Epoch [470/1000], Loss: 1.5853\n",
            "Epoch [480/1000], Loss: 1.5150\n",
            "Epoch [490/1000], Loss: 1.4520\n",
            "Epoch [500/1000], Loss: 1.3831\n",
            "Epoch [510/1000], Loss: 1.3171\n",
            "Epoch [520/1000], Loss: 1.2595\n",
            "Epoch [530/1000], Loss: 1.2084\n",
            "Epoch [540/1000], Loss: 1.1388\n",
            "Epoch [550/1000], Loss: 1.0851\n",
            "Epoch [560/1000], Loss: 1.0415\n",
            "Epoch [570/1000], Loss: 0.9843\n",
            "Epoch [580/1000], Loss: 0.9393\n",
            "Epoch [590/1000], Loss: 0.8865\n",
            "Epoch [600/1000], Loss: 0.8436\n",
            "Epoch [610/1000], Loss: 0.8030\n",
            "Epoch [620/1000], Loss: 0.7632\n",
            "Epoch [630/1000], Loss: 0.7308\n",
            "Epoch [640/1000], Loss: 0.7033\n",
            "Epoch [650/1000], Loss: 0.6679\n",
            "Epoch [660/1000], Loss: 0.6437\n",
            "Epoch [670/1000], Loss: 0.6134\n",
            "Epoch [680/1000], Loss: 0.5918\n",
            "Epoch [690/1000], Loss: 0.5703\n",
            "Epoch [700/1000], Loss: 0.5492\n",
            "Epoch [710/1000], Loss: 0.5325\n",
            "Epoch [720/1000], Loss: 0.5149\n",
            "Epoch [730/1000], Loss: 0.5012\n",
            "Epoch [740/1000], Loss: 0.4870\n",
            "Epoch [750/1000], Loss: 0.4743\n",
            "Epoch [760/1000], Loss: 0.4654\n",
            "Epoch [770/1000], Loss: 0.4552\n",
            "Epoch [780/1000], Loss: 0.4421\n",
            "Epoch [790/1000], Loss: 0.4315\n",
            "Epoch [800/1000], Loss: 0.4263\n",
            "Epoch [810/1000], Loss: 0.4149\n",
            "Epoch [820/1000], Loss: 0.4085\n",
            "Epoch [830/1000], Loss: 0.4013\n",
            "Epoch [840/1000], Loss: 0.3940\n",
            "Epoch [850/1000], Loss: 0.3883\n",
            "Epoch [860/1000], Loss: 0.3829\n",
            "Epoch [870/1000], Loss: 0.3770\n",
            "Epoch [880/1000], Loss: 0.3719\n",
            "Epoch [890/1000], Loss: 0.3672\n",
            "Epoch [900/1000], Loss: 0.3627\n",
            "Epoch [910/1000], Loss: 0.3587\n",
            "Epoch [920/1000], Loss: 0.3549\n",
            "Epoch [930/1000], Loss: 0.3522\n",
            "Epoch [940/1000], Loss: 0.3474\n",
            "Epoch [950/1000], Loss: 0.3445\n",
            "Epoch [960/1000], Loss: 0.3411\n",
            "Epoch [970/1000], Loss: 0.3395\n",
            "Epoch [980/1000], Loss: 0.3350\n",
            "Epoch [990/1000], Loss: 0.3319\n",
            "Epoch [1000/1000], Loss: 0.3297\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(loss)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "HkLB2-85sAmv",
        "outputId": "203698e2-0176-429f-d016-d0d838d51078"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds = model(inputs)\n",
        "preds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nq3mRhwpklRo",
        "outputId": "0793c4c8-3834-4fd7-a806-30437c09edd4"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.5729],\n",
              "        [0.6791],\n",
              "        [0.9026],\n",
              "        [0.1569],\n",
              "        [0.8761],\n",
              "        [0.5729],\n",
              "        [0.6791],\n",
              "        [0.9026],\n",
              "        [0.1569],\n",
              "        [0.8761],\n",
              "        [0.5729],\n",
              "        [0.6791],\n",
              "        [0.9026],\n",
              "        [0.1569],\n",
              "        [0.8761]], grad_fn=<SigmoidBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "targets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RqzvWKcvknwr",
        "outputId": "cb3f5fbb-1590-4137-ea95-a70a3e0f8191"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Модель значно поліпшилась. З графіку видно, що після 500 епох значення помилки вже не так сильно змінюється. При порівнянні прогнозованих значень та цільових міток, бачимо, що модель досить непогано зробила передбачення."
      ],
      "metadata": {
        "id": "lsGn3vgtkyC_"
      }
    }
  ]
}